{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying to install Jupyter nb. The default python3.6 and pip3 are broken and I can't use it to install Jupyter nb. \n",
    "# Am fixing this. I fixed this by using the offical python3.12 installer and running `Update Shell Profile.command` afterwards\n",
    "# However, I am running python3.9 instead of python3.12\n",
    "\n",
    "# But in this Jupyter lab, I am running 3.12\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "  return 3*x**2 - 4*x + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([100.    ,  91.6875,  83.75  ,  76.1875,  69.    ,  62.1875,\n",
       "        55.75  ,  49.6875,  44.    ,  38.6875,  33.75  ,  29.1875,\n",
       "        25.    ,  21.1875,  17.75  ,  14.6875,  12.    ,   9.6875,\n",
       "         7.75  ,   6.1875,   5.    ,   4.1875,   3.75  ,   3.6875,\n",
       "         4.    ,   4.6875,   5.75  ,   7.1875,   9.    ,  11.1875,\n",
       "        13.75  ,  16.6875,  20.    ,  23.6875,  27.75  ,  32.1875,\n",
       "        37.    ,  42.1875,  47.75  ,  53.6875])"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs = np.arange(-5, 5, 0.25)\n",
    "ys = f(xs)\n",
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x16d1dd220>]"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(xs, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.000000000026205\n"
     ]
    }
   ],
   "source": [
    "# derivative of a function with multiple inputs\n",
    "a = 1\n",
    "b = 2\n",
    "c = 3\n",
    "d1 = 2*a*b - c\n",
    "\n",
    "h = 0.00001\n",
    "\n",
    "# the derivative of d with respect to a is:\n",
    "a += h\n",
    "d2 = 2*a*b - c\n",
    "print((d2-d1)/h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=-8.0)"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# starting the core Value object of micrograd\n",
    "class Value():\n",
    "  def __init__(self, data: float, _children=(), _op=\"\", label=\"\"):\n",
    "    self.data = data\n",
    "    self.grad = 0 # by default, we assume the gradient is 0\n",
    "    self._backward = lambda : None\n",
    "    self._prev = set(_children)\n",
    "    self._op = _op\n",
    "    self.label = label\n",
    "  \n",
    "  def __repr__(self) -> str:\n",
    "    return f\"Value(data={self.data})\"\n",
    "\n",
    "  def __add__(self, other):\n",
    "    other = other if isinstance(other, Value) else Value(other)\n",
    "    out = Value(self.data + other.data, (self, other), \"+\")\n",
    "\n",
    "    def _backward():\n",
    "      # Using += instead of = to accumulate the gradient when the param is used multiple times\n",
    "      # implying it has multiple avenues to influence L\n",
    "      self.grad += out.grad  \n",
    "      other.grad += out.grad\n",
    "    out._backward = _backward\n",
    "\n",
    "    return out\n",
    "\n",
    "  def __mul__(self, other):\n",
    "    other = other if isinstance(other, Value) else Value(other)\n",
    "    out = Value(self.data * other.data, (self, other), \"*\")\n",
    "\n",
    "    def _backward():\n",
    "      self.grad += other.data * out.grad\n",
    "      other.grad += self.data * out.grad\n",
    "    out._backward = _backward\n",
    "\n",
    "    return out\n",
    "\n",
    "  def __rmul__(self, other): # other * self\n",
    "    return self * other\n",
    "  \n",
    "  def __pow__(self, other):\n",
    "    assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n",
    "    out = Value(self.data**other, (self,), f'**{other}')\n",
    "\n",
    "    def _backward():\n",
    "        self.grad += other * (self.data ** (other - 1)) * out.grad\n",
    "    out._backward = _backward\n",
    "\n",
    "    return out\n",
    "\n",
    "  def __neg__(self): # -self\n",
    "    return self * -1\n",
    "\n",
    "  def __sub__(self, other): # self - other\n",
    "    return self + (-other)\n",
    "\n",
    "  def __rsub__(self, other): # other - self\n",
    "    return other + (-self)\n",
    "\n",
    "  def __radd__(self, other): # other + self\n",
    "    return self + other\n",
    "\n",
    "  def tanh(self):\n",
    "    x = self.data\n",
    "    t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n",
    "    out = Value(t, (self, ), 'tanh')\n",
    "    \n",
    "    def _backward():\n",
    "      self.grad += (1-t**2) * out.grad\n",
    "    out._backward = _backward\n",
    "\n",
    "    return out\n",
    "\n",
    "  \n",
    "  def backward(self):\n",
    "    \n",
    "    topo = []\n",
    "    visited = set()\n",
    "    def build_topo(v):\n",
    "      if v not in visited:\n",
    "        visited.add(v)\n",
    "        for child in v._prev:\n",
    "          build_topo(child)\n",
    "        topo.append(v)\n",
    "    build_topo(self)\n",
    "    \n",
    "    self.grad = 1.0\n",
    "    for node in reversed(topo):\n",
    "      node._backward()\n",
    "\n",
    "a = Value(2.0, label='a')\n",
    "b = Value(-3.0, label='b')\n",
    "c = Value(10.0, label='c')\n",
    "e = a*b; e.label = 'e'\n",
    "d = e + c; d.label = 'd'\n",
    "f = Value(-2.0, label='f')\n",
    "L = d * f; L.label = 'L'\n",
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual gradient calc\n",
    "a.grad = 6.0\n",
    "b.grad = -4.0\n",
    "c.grad = -2.0\n",
    "e.grad = -2.0\n",
    "d.grad = -2.0\n",
    "f.grad = 4.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def gradient_checking():\n",
    "  h = 0.0001\n",
    "\n",
    "  a = Value(2.0, label='a')\n",
    "  b = Value(-3.0, label='b')\n",
    "  c = Value(10.0, label='c')\n",
    "  e = a*b; e.label = 'e'\n",
    "  d = e + c; d.label = 'd'\n",
    "  f = Value(-2.0, label='f')\n",
    "  L = d * f; L.label = 'L'\n",
    "  L1 = L.data\n",
    "\n",
    "  a = Value(2.0, label='a')\n",
    "  b = Value(-3.0, label='b')\n",
    "  c = Value(10.0, label='c')\n",
    "  e = a*b; e.label = 'e'\n",
    "  d = e + c; d.label = 'd'\n",
    "  f = Value(-2.0, label='f')\n",
    "  L = d * f; L.label = 'L'\n",
    "  L2 = L.data\n",
    "\n",
    "  print((L2-L1)/h)\n",
    "\n",
    "gradient_checking()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-7.286496\n"
     ]
    }
   ],
   "source": [
    "# One optimization step\n",
    "# note that only free parameters receive an addition of the epsilon\n",
    "a.data += 0.01 * a.grad\n",
    "b.data += 0.01 * b.grad\n",
    "c.data += 0.01 * c.grad\n",
    "f.data += 0.01 * f.grad\n",
    "\n",
    "e = a * b\n",
    "d = e + c\n",
    "L = d * f\n",
    "\n",
    "print(L.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing a neuron with Value object\n",
    "# inputs x1,x2\n",
    "x1 = Value(2.0, label='x1')\n",
    "x2 = Value(0.0, label='x2')\n",
    "# weights w1,w2\n",
    "w1 = Value(-3.0, label='w1')\n",
    "w2 = Value(1.0, label='w2')\n",
    "# bias of the neuron\n",
    "b = Value(6.8813735870195432, label='b')\n",
    "# x1*w1 + x2*w2 + b\n",
    "x1w1 = x1*w1; x1w1.label = 'x1*w1'\n",
    "x2w2 = x2*w2; x2w2.label = 'x2*w2'\n",
    "x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1*w1 + x2*w2'\n",
    "n = x1w1x2w2 + b; n.label = 'n'\n",
    "o = n.tanh(); o.label = 'o'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again manually compute the gradient for all parameters we will be able to see the impact each inputs have on `o`. \n",
    "# Intermediate values are labeled like `x1w1x2w2` to differentiate themselves from input values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step-wise automation of gradient computation\n",
    "o.grad = 1.0\n",
    "o._backward()\n",
    "n._backward()\n",
    "b._backward()\n",
    "x1w1x2w2._backward()\n",
    "x2w2._backward()\n",
    "x1w1._backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7071066904050358\n",
      "---\n",
      "x2 0.5000001283844369\n",
      "w2 0.0\n",
      "x1 -1.5000003851533106\n",
      "w1 1.0000002567688737\n"
     ]
    }
   ],
   "source": [
    "# Building the same structure but using PyTorch\n",
    "# Note that I have installed the nightly version because stable PyTorch doesn't support python3.12 yet\n",
    "import torch\n",
    "\n",
    "x1 = torch.Tensor([2.]).double(); x1.requires_grad = True\n",
    "x2 = torch.Tensor([0.]).double(); x2.requires_grad = True\n",
    "w1 = torch.Tensor([-3.]).double(); w1.requires_grad = True\n",
    "w2 = torch.Tensor([1.]).double(); w2.requires_grad = True\n",
    "b = torch.Tensor([6.8813735870195432]).double(); b.requires_grad = True\n",
    "n = x1*w1 + x2*w2 + b\n",
    "o = torch.tanh(n)\n",
    "\n",
    "print(o.item())\n",
    "o.backward()\n",
    "\n",
    "print('---')\n",
    "print('x2', x2.grad.item())\n",
    "print('w2', w2.grad.item())\n",
    "print('x1', x1.grad.item())\n",
    "print('w1', w1.grad.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=-0.6723088554972202)"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "class Neuron:\n",
    "  def __init__(self, nin):\n",
    "    self.w = [Value(random.uniform(-1, 1)) for _ in range(nin)]\n",
    "    self.b = Value(random.uniform(-1, 1))\n",
    "\n",
    "  def __call__(self, x: list):\n",
    "    s = sum([wi*xi for wi, xi in zip(self.w, x)], self.b)\n",
    "    out = s.tanh()\n",
    "    return out\n",
    "\n",
    "  def parameters(self):\n",
    "    return self.w + [self.b]\n",
    "\n",
    "x = [2.0, 3.0]\n",
    "n = Neuron(2)\n",
    "n(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=-0.9760932006623869),\n",
       " Value(data=-0.5582268112345695),\n",
       " Value(data=-0.9979127250571561)]"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Layer:\n",
    "  def __init__(self, nin, nn) -> None:\n",
    "    self.layer = [Neuron(nin) for _ in range(nn)]\n",
    "\n",
    "  def __call__(self, x):\n",
    "    outs = [neuron(x) for neuron in self.layer]\n",
    "    return outs[0] if len(outs) == 1 else outs\n",
    "\n",
    "  def parameters(self):\n",
    "    return [p for neuron in self.layer for p in neuron.parameters()]\n",
    "\n",
    "x = [2.0, 3.0]\n",
    "l = Layer(2, 3)\n",
    "l(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=0.870682297577472)"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is my implementation: note that Andrej's implementation wrap `nl`, `nn` and `no` in one \n",
    "# single param `nouts`, is more flexible because number of neurons in each layer is expressed\n",
    "# and thus no assumption such as mine needed to be made.\n",
    "# In conclusion, his implementation is more general\n",
    "class MY_MLP_IMPLEM:\n",
    "  def __init__(self, ni, nl, nn, no = 1):\n",
    "    # assuming the number of neurons in all hidden layers is the same\n",
    "    self.nl = nl\n",
    "    self.layers = [Layer(nn if i!=0 else ni, nn) for i in range(nl)]\n",
    "    self.layers += [Layer(nn, no)]\n",
    "\n",
    "  def __call__(self, x):\n",
    "    # a forward pass\n",
    "    out = x\n",
    "    for layer in self.layers:\n",
    "      out = layer(out)\n",
    "    return out\n",
    "  \n",
    "x = [2., 3., -1.]\n",
    "n = MY_MLP_IMPLEM(3, 2, 4)\n",
    "n(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "  def __init__(self, nin, nns):\n",
    "    sz = [nin] + nns\n",
    "    self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nns))]\n",
    "\n",
    "  def __call__(self, x):\n",
    "    # a forward pass\n",
    "    out = x\n",
    "    for layer in self.layers:\n",
    "      out = layer(out)\n",
    "    return out\n",
    "\n",
    "  def parameters(self):\n",
    "    return [p for layer in self.layers for p in layer.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=-0.163314708230921)"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize the network with one example, and do a forward pass\n",
    "x = [2., 3., -1.]\n",
    "n = MLP(3, [4,4,1])\n",
    "n(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=-0.163314708230921),\n",
       " Value(data=-0.024166038036785728),\n",
       " Value(data=-0.09888440903562773),\n",
       " Value(data=0.24930280739531968)]"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a tiny dataset and do a forward pass on each example in the set\n",
    "xs = [\n",
    "  [2.0, 3.0, -1.0],\n",
    "  [3.0, -1.0, 0.5],\n",
    "  [0.5, 1.0, 1.0],\n",
    "  [1.0, 1.0, -1.0],\n",
    "]\n",
    "ys = [1.0, -1.0, -1.0, 1.0] # desired targets\n",
    "ypred = [n(x) for x in xs]\n",
    "ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=3.681108614970835)"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Writing the loss function\n",
    "loss = sum((ygt - yout)**2 for ygt, yout in zip(ys, ypred))\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(n.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.681108614970835\n",
      "2.8116630774021583\n",
      "2.1606996471891544\n",
      "1.7252794465842372\n",
      "1.4316357882813286\n",
      "1.2227962201782123\n",
      "1.0667196915335295\n",
      "0.9453521560710988\n",
      "0.8480110127432197\n",
      "0.7680386610022789\n",
      "0.7010792339071562\n",
      "0.6441569025157367\n",
      "0.5951615792170107\n",
      "0.5525492500160771\n",
      "0.5151598952035867\n",
      "0.48210245552372377\n",
      "0.4526795379526143\n",
      "0.4263365525596441\n",
      "0.40262638305358267\n",
      "0.38118424002802914\n",
      "0.36170937473763076\n",
      "0.3439515296324841\n",
      "0.3277007313401202\n",
      "0.3127794883586805\n",
      "0.2990367489269271\n",
      "0.28634316730190157\n",
      "0.2745873561543311\n",
      "0.26367289147610223\n",
      "0.25351589822430864\n",
      "0.24404308873576172\n",
      "0.2351901574466186\n",
      "0.22690045840815826\n",
      "0.21912390902596746\n",
      "0.21181607608604802\n",
      "0.20493740965750284\n",
      "0.19845259771188162\n",
      "0.19233001986659828\n",
      "0.18654128297003336\n",
      "0.181060824608394\n",
      "0.17586557325633806\n",
      "0.1709346558830327\n",
      "0.1662491454885769\n",
      "0.16179184237738348\n",
      "0.15754708404725193\n",
      "0.15350057944060022\n",
      "0.14963926401011513\n",
      "0.14595117262788565\n",
      "0.14242532784058734\n",
      "0.13905164136364595\n",
      "0.13582082703045875\n",
      "0.1327243236812777\n",
      "0.12975422670035883\n",
      "0.12690322709747703\n",
      "0.12416455718739908\n",
      "0.12153194205363273\n",
      "0.1189995560949656\n",
      "0.11656198404845018\n",
      "0.11421418596339497\n",
      "0.1119514656699261\n",
      "0.10976944234467922\n",
      "0.10766402482677619\n",
      "0.10563138838071393\n",
      "0.10366795364026551\n",
      "0.10177036749984208\n",
      "0.0999354857477805\n",
      "0.09816035726031802\n",
      "0.09644220959614336\n",
      "0.0947784358498195\n",
      "0.09316658263844237\n",
      "0.09160433910995708\n",
      "0.0900895268738653\n",
      "0.08862009076587915\n",
      "0.08719409036758322\n",
      "0.08580969221055011\n",
      "0.08446516260175252\n",
      "0.08315886101365097\n",
      "0.08188923398813173\n",
      "0.08065480950859907\n",
      "0.07945419179909247\n",
      "0.07828605651335294\n",
      "0.07714914628038001\n",
      "0.07604226657624144\n",
      "0.07496428189478493\n",
      "0.07391411219246795\n",
      "0.0728907295848405\n",
      "0.07189315527427595\n",
      "0.07092045669041075\n",
      "0.06997174482642035\n",
      "0.06904617175576555\n",
      "0.06814292831539759\n",
      "0.0672612419426349\n",
      "0.06640037465402734\n",
      "0.06555962115552343\n",
      "0.06473830707416331\n",
      "0.0639357873023336\n",
      "0.06315144444636843\n",
      "0.062384687371955905\n",
      "0.061634949839419174\n",
      "0.06090168922250279\n",
      "0.06018438530480145\n",
      "0.05948253914843306\n",
      "0.058795672029978444\n",
      "0.05812332443909897\n",
      "0.057465055135592163\n",
      "0.05682044026097325\n",
      "0.05618907250096011\n",
      "0.055570560295514895\n",
      "0.05496452709334029\n",
      "0.05437061064795769\n",
      "0.053788462352704264\n",
      "0.05321774661217611\n",
      "0.052658140247823566\n",
      "0.05210933193556733\n",
      "0.05157102167345268\n",
      "0.05104292027749828\n",
      "0.05052474890402426\n",
      "0.0500162385968608\n",
      "0.049517129857947276\n",
      "0.04902717223993424\n",
      "0.04854612395949054\n",
      "0.048073751530107955\n",
      "0.04760982941327049\n",
      "0.04715413968693462\n",
      "0.04670647173033089\n",
      "0.04626662192416413\n",
      "0.045834393365345893\n",
      "0.04540959559544973\n",
      "0.04499204434212983\n",
      "0.044581561272790525\n",
      "0.04417797375983998\n",
      "0.043781114656900204\n",
      "0.043390822085385757\n",
      "0.04300693923089828\n",
      "0.0426293141489166\n",
      "0.04225779957929503\n",
      "0.04189225276910991\n",
      "0.04153253530342154\n",
      "0.04117851294354491\n",
      "0.040830055472445795\n",
      "0.04048703654690064\n",
      "0.040149333556078866\n",
      "0.039816827486228085\n",
      "0.039489402791156814\n",
      "0.0391669472682306\n",
      "0.03884935193961016\n",
      "0.038536510938476626\n",
      "0.038228321400002416\n",
      "0.037924683356840745\n",
      "0.03762549963891653\n",
      "0.037330675777316094\n",
      "0.03704011991208216\n",
      "0.03675374270373061\n",
      "0.03647145724831708\n",
      "0.03619317899588855\n",
      "0.03591882567216472\n",
      "0.035648317203301486\n",
      "0.03538157564359755\n",
      "0.035118525106010635\n",
      "0.03485909169535741\n",
      "0.03460320344407892\n",
      "0.034350790250456044\n",
      "0.03410178381916931\n",
      "0.03385611760409854\n",
      "0.03361372675326679\n",
      "0.03337454805583477\n",
      "0.03313851989105794\n",
      "0.03290558217912305\n",
      "0.032675676333783675\n",
      "0.03244874521671889\n",
      "0.03222473309354341\n",
      "0.03200358559139955\n",
      "0.03178524965806594\n",
      "0.03156967352252015\n",
      "0.03135680665689569\n",
      "0.03114659973977654\n",
      "0.030939004620775076\n",
      "0.030733974286341274\n",
      "0.030531462826754265\n",
      "0.030331425404249192\n",
      "0.030133818222233003\n",
      "0.02993859849554876\n",
      "0.02974572442174442\n",
      "0.02955515515330911\n",
      "0.029366850770838247\n",
      "0.029180772257092216\n",
      "0.028996881471913274\n",
      "0.02881514112796936\n",
      "0.028635514767292328\n",
      "0.028457966738580615\n",
      "0.02828246217523847\n",
      "0.028108966974122705\n",
      "0.027937447774972507\n",
      "0.027767871940494755\n",
      "0.027600207537083248\n",
      "0.02743442331614631\n",
      "0.027270488696021798\n",
      "0.027108373744458174\n",
      "0.02694804916164069\n",
      "0.02678948626374348\n",
      "0.026632656966988903\n",
      "0.02647753377219496\n",
      "0.02632408974979574\n",
      "0.026172298525315517\n",
      "0.026022134265282673\n",
      "0.025873571663567442\n",
      "0.025726585928128067\n",
      "0.025581152768152392\n",
      "0.025437248381580367\n",
      "0.025294849442995367\n",
      "0.02515393309187039\n",
      "0.02501447692115865\n",
      "0.024876458966215598\n",
      "0.0247398576940423\n",
      "0.02460465199283848\n",
      "0.024470821161855814\n",
      "0.024338344901540902\n",
      "0.024207203303958348\n",
      "0.024077376843485356\n",
      "0.023948846367767992\n",
      "0.023821593088931947\n",
      "0.023695598575038078\n",
      "0.02357084474177553\n",
      "0.02344731384438555\n",
      "0.023324988469806747\n",
      "0.023203851529036888\n",
      "0.02308388624970259\n",
      "0.022965076168831658\n",
      "0.02284740512582057\n",
      "0.022730857255592256\n",
      "0.02261541698193692\n",
      "0.022501069011031213\n",
      "0.022387798325129923\n",
      "0.022275590176424654\n",
      "0.022164430081064765\n",
      "0.022054303813335417\n",
      "0.021945197399987914\n",
      "0.021837097114717673\n",
      "0.02172998947278588\n",
      "0.021623861225779948\n",
      "0.02151869935650849\n",
      "0.02141449107402762\n",
      "0.021311223808794348\n",
      "0.021208885207942372\n",
      "0.021107463130678333\n",
      "0.021006945643793802\n",
      "0.02090732101728984\n",
      "0.02080857772011154\n",
      "0.020710704415988115\n",
      "0.02061368995937666\n",
      "0.020517523391506316\n",
      "0.02042219393651941\n",
      "0.020327690997707447\n",
      "0.02023400415383892\n",
      "0.02014112315557689\n",
      "0.02004903792198267\n",
      "0.019957738537104454\n",
      "0.01986721524664784\n",
      "0.019777458454726234\n",
      "0.019688458720688386\n",
      "0.019600206756021924\n",
      "0.019512693421329688\n",
      "0.019425909723377946\n",
      "0.019339846812213412\n",
      "0.019254495978348184\n",
      "0.01916984865001021\n",
      "0.01908589639045771\n",
      "0.019002630895355522\n",
      "0.01892004399021254\n",
      "0.018838127627877455\n",
      "0.018756873886092013\n",
      "0.018676274965100395\n",
      "0.018596323185312593\n",
      "0.018517010985020753\n",
      "0.018438330918167267\n",
      "0.01836027565216279\n",
      "0.018282837965753293\n",
      "0.018206010746934762\n",
      "0.0181297869909141\n",
      "0.018054159798115466\n",
      "0.01797912237223043\n",
      "0.017904668018311178\n",
      "0.017830790140905557\n",
      "0.01775748224223256\n",
      "0.01768473792039824\n",
      "0.017612550867649422\n",
      "0.017540914868665915\n",
      "0.017469823798889245\n",
      "0.01739927162288722\n",
      "0.017329252392753697\n",
      "0.01725976024654218\n",
      "0.01719078940673313\n",
      "0.01712233417873331\n",
      "0.017054388949407112\n",
      "0.016986948185638635\n",
      "0.01692000643292397\n",
      "0.016853558313992996\n",
      "0.016787598527459632\n",
      "0.016722121846500376\n",
      "0.01665712311755997\n",
      "0.01659259725908408\n",
      "0.01652853926027749\n",
      "0.016464944179888204\n",
      "0.016401807145016357\n",
      "0.016339123349947006\n",
      "0.016276888055006877\n",
      "0.016215096585444247\n",
      "0.01615374433033134\n",
      "0.016092826741488635\n",
      "0.016032339332430902\n",
      "0.015972277677334265\n",
      "0.015912637410023647\n",
      "0.015853414222980623\n",
      "0.01579460386637032\n",
      "0.01573620214708826\n",
      "0.015678204927825186\n",
      "0.015620608126150726\n",
      "0.015563407713614613\n",
      "0.015506599714865767\n",
      "0.01545018020678781\n",
      "0.015394145317652244\n",
      "0.015338491226286935\n",
      "0.015283214161261528\n",
      "0.01522831040008763\n",
      "0.01517377626843484\n",
      "0.015119608139361564\n",
      "0.015065802432560232\n",
      "0.015012355613617127\n",
      "0.014959264193286012\n",
      "0.014906524726775122\n",
      "0.014854133813048226\n",
      "0.014802088094138213\n",
      "0.014750384254473482\n",
      "0.014699019020217207\n",
      "0.01464798915861849\n",
      "0.014597291477375724\n",
      "0.01454692282401149\n",
      "0.014496880085259229\n",
      "0.014447160186460872\n",
      "0.014397760090975362\n",
      "0.014348676799598543\n",
      "0.014299907349992909\n",
      "0.014251448816127981\n",
      "0.014203298307730847\n",
      "0.014155452969746294\n",
      "0.014107909981807047\n",
      "0.014060666557713142\n",
      "0.014013719944920698\n",
      "0.013967067424039787\n",
      "0.01392070630834135\n",
      "0.013874633943272499\n",
      "0.013828847705980672\n",
      "0.013783345004846063\n",
      "0.013738123279022227\n",
      "0.013693179997984594\n",
      "0.013648512661087298\n",
      "0.013604118797127144\n",
      "0.013559995963915402\n",
      "0.013516141747857247\n",
      "0.013472553763537967\n",
      "0.01342922965331652\n",
      "0.013386167086926314\n",
      "0.013343363761082373\n",
      "0.013300817399095343\n",
      "0.013258525750492356\n",
      "0.01321648659064387\n",
      "0.01317469772039717\n",
      "0.013133156965715815\n",
      "0.013091862177325397\n",
      "0.013050811230364999\n",
      "0.013010002024044592\n",
      "0.012969432481308139\n",
      "0.01292910054850251\n",
      "0.012889004195051408\n",
      "0.01284914141313518\n",
      "0.0128095102173756\n",
      "0.012770108644526008\n",
      "0.012730934753166662\n",
      "0.012691986623404521\n",
      "0.012653262356578722\n",
      "0.012614760074970175\n",
      "0.01257647792151631\n",
      "0.01253841405953027\n",
      "0.012500566672424605\n",
      "0.012462933963439459\n",
      "0.012425514155375399\n",
      "0.012388305490329958\n",
      "0.012351306229438891\n",
      "0.012314514652621431\n",
      "0.01227792905832944\n",
      "0.012241547763300675\n",
      "0.012205369102316153\n",
      "0.012169391427960736\n",
      "0.012133613110388394\n",
      "0.012098032537090204\n",
      "0.012062648112666727\n",
      "0.012027458258603534\n",
      "0.011992461413050472\n",
      "0.011957656030604148\n",
      "0.011923040582093786\n",
      "0.01188861355437077\n",
      "0.01185437345010102\n",
      "0.011820318787560673\n",
      "0.011786448100435018\n",
      "0.011752759937620514\n",
      "0.01171925286302946\n",
      "0.011685925455398382\n",
      "0.011652776308098647\n",
      "0.01161980402895008\n",
      "0.01158700724003787\n",
      "0.011554384577531671\n",
      "0.011521934691507581\n",
      "0.011489656245773167\n",
      "0.011457547917694554\n",
      "0.011425608398026473\n",
      "0.011393836390744767\n",
      "0.011362230612881198\n",
      "0.011330789794361137\n",
      "0.011299512677843189\n",
      "0.011268398018561419\n",
      "0.01123744458416993\n",
      "0.011206651154589655\n",
      "0.011176016521857438\n",
      "0.011145539489977208\n",
      "0.011115218874773566\n",
      "0.011085053503747201\n",
      "0.011055042215932773\n",
      "0.011025183861758417\n",
      "0.010995477302907678\n",
      "0.010965921412183222\n",
      "0.01093651507337258\n",
      "0.010907257181115751\n",
      "0.01087814664077488\n",
      "0.01084918236830554\n",
      "0.010820363290130006\n",
      "0.010791688343012449\n",
      "0.01076315647393557\n",
      "0.010734766639979403\n",
      "0.010706517808201386\n",
      "0.010678408955518453\n",
      "0.01065043906859069\n",
      "0.010622607143706756\n",
      "0.010594912186670339\n",
      "0.010567353212689071\n",
      "0.010539929246264396\n",
      "0.010512639321083029\n",
      "0.010485482479910161\n",
      "0.010458457774483919\n",
      "0.010431564265411294\n",
      "0.010404801022065889\n",
      "0.010378167122486341\n",
      "0.010351661653276787\n",
      "0.01032528370950859\n",
      "0.010299032394622914\n",
      "0.01027290682033542\n",
      "0.010246906106541432\n",
      "0.010221029381223193\n",
      "0.010195275780357695\n",
      "0.010169644447826082\n",
      "0.010144134535324402\n",
      "0.010118745202275192\n",
      "0.010093475615740553\n",
      "0.010068324950336355\n",
      "0.010043292388147336\n",
      "0.010018377118643682\n",
      "0.00999357833859853\n",
      "0.009968895252006497\n",
      "0.0099443270700036\n",
      "0.009919873010787741\n",
      "0.009895532299540781\n",
      "0.00987130416835125\n",
      "0.00984718785613815\n",
      "0.009823182608576085\n",
      "0.009799287678020765\n",
      "0.009775502323436058\n",
      "0.009751825810321628\n",
      "0.009728257410641688\n",
      "0.009704796402754644\n",
      "0.009681442071343718\n",
      "0.009658193707348163\n",
      "0.009635050607895781\n",
      "0.009612012076236083\n",
      "0.009589077421674353\n",
      "0.009566245959506442\n",
      "0.009543517010954641\n",
      "0.009520889903104231\n",
      "0.009498363968840776\n",
      "0.009475938546788246\n",
      "0.009453612981248074\n",
      "0.009431386622138842\n",
      "0.009409258824936727\n",
      "0.009387228950616711\n",
      "0.009365296365594692\n",
      "0.009343460441669985\n",
      "0.009321720555968936\n",
      "0.009300076090888875\n",
      "0.009278526434043047\n",
      "0.009257070978206062\n",
      "0.009235709121260259\n",
      "0.009214440266142213\n",
      "0.009193263820790649\n",
      "0.009172179198094245\n",
      "0.009151185815840793\n",
      "0.009130283096666308\n",
      "0.009109470468005306\n",
      "0.009088747362041353\n",
      "0.00906811321565836\n",
      "0.009047567470392488\n",
      "0.009027109572384596\n",
      "0.009006738972333247\n",
      "0.008986455125448291\n",
      "0.008966257491405115\n",
      "0.00894614553429928\n",
      "0.008926118722601791\n",
      "0.008906176529114924\n",
      "0.008886318430928668\n",
      "0.008866543909377363\n",
      "0.008846852449997219\n",
      "0.008827243542484231\n",
      "0.008807716680652463\n",
      "0.00878827136239307\n",
      "0.00876890708963352\n",
      "0.008749623368297502\n",
      "0.008730419708265394\n",
      "0.008711295623334822\n",
      "0.00869225063118208\n",
      "0.008673284253323896\n",
      "0.008654396015079436\n",
      "0.008635585445532999\n",
      "0.008616852077497121\n",
      "0.008598195447475974\n",
      "0.008579615095629326\n",
      "0.008561110565736826\n",
      "0.008542681405162752\n",
      "0.00852432716482123\n",
      "0.008506047399141724\n",
      "0.008487841666034966\n",
      "0.008469709526859373\n",
      "0.008451650546387721\n",
      "0.00843366429277434\n",
      "0.008415750337522461\n",
      "0.008397908255452233\n",
      "0.008380137624668889\n",
      "0.00836243802653131\n",
      "0.00834480904562103\n",
      "0.008327250269711517\n",
      "0.008309761289737806\n",
      "0.008292341699766552\n",
      "0.00827499109696632\n",
      "0.008257709081578253\n",
      "0.008240495256887094\n",
      "0.008223349229192425\n",
      "0.00820627060778056\n",
      "0.008189259004896152\n",
      "0.00817231403571476\n",
      "0.008155435318315345\n",
      "0.008138622473653146\n",
      "0.008121875125532824\n",
      "0.008105192900582077\n",
      "0.008088575428225345\n",
      "0.008072022340657892\n",
      "0.008055533272820204\n",
      "0.008039107862372562\n",
      "0.008022745749670076\n",
      "0.008006446577737745\n",
      "0.007990209992246089\n",
      "0.007974035641486686\n",
      "0.007957923176348412\n",
      "0.007941872250293478\n",
      "0.007925882519334166\n",
      "0.00790995364200935\n",
      "0.00789408527936186\n",
      "0.007878277094915462\n",
      "0.007862528754652534\n",
      "0.00784683992699175\n",
      "0.007831210282766267\n",
      "0.007815639495201686\n",
      "0.0078001272398947686\n",
      "0.007784673194792063\n",
      "0.007769277040168774\n",
      "0.007753938458608002\n",
      "0.007738657134980176\n",
      "0.007723432756422484\n",
      "0.007708265012318817\n",
      "0.007693153594279817\n",
      "0.007678098196123076\n",
      "0.007663098513853517\n",
      "0.007648154245644209\n",
      "0.00763326509181713\n",
      "0.007618430754824174\n",
      "0.007603650939228632\n",
      "0.007588925351686376\n",
      "0.007574253700927678\n",
      "0.0075596356977390535\n",
      "0.007545071054945226\n",
      "0.007530559487391379\n",
      "0.007516100711925584\n",
      "0.007501694447381279\n",
      "0.0074873404145603326\n",
      "0.007473038336215555\n",
      "0.007458787937034137\n",
      "0.007444588943620907\n",
      "0.007430441084481651\n",
      "0.007416344090006893\n",
      "0.0074022976924556505\n",
      "0.007388301625939479\n",
      "0.007374355626406407\n",
      "0.007360459431625408\n",
      "0.007346612781170897\n",
      "0.007332815416407142\n",
      "0.007319067080473253\n",
      "0.007305367518267925\n",
      "0.007291716476434638\n",
      "0.007278113703346828\n",
      "0.007264558949093286\n",
      "0.007251051965463749\n",
      "0.007237592505934392\n",
      "0.007224180325653804\n",
      "0.007210815181428892\n",
      "0.007197496831710979\n",
      "0.007184225036581969\n",
      "0.00717099955774096\n",
      "0.0071578201584903585\n",
      "0.007144686603722898\n",
      "0.007131598659908218\n",
      "0.007118556095079795\n",
      "0.007105558678822012\n",
      "0.007092606182257294\n",
      "0.007079698378033419\n",
      "0.007066835040310922\n",
      "0.0070540159447506695\n",
      "0.007041240868501381\n",
      "0.007028509590187635\n",
      "0.007015821889897636\n",
      "0.0070031775491712\n",
      "0.006990576350987921\n",
      "0.006978018079755578\n",
      "0.006965502521298208\n",
      "0.006953029462844837\n",
      "0.006940598693017995\n",
      "0.0069282100018223484\n",
      "0.006915863180633612\n",
      "0.006903558022187349\n",
      "0.006891294320568149\n",
      "0.006879071871198614\n",
      "0.006866890470828691\n",
      "0.0068547499175250146\n",
      "0.006842650010660171\n",
      "0.006830590550902524\n",
      "0.006818571340205637\n",
      "0.006806592181798078\n",
      "0.006794652880173299\n",
      "0.006782753241079519\n",
      "0.006770893071509692\n",
      "0.006759072179691846\n",
      "0.006747290375079093\n",
      "0.006735547468339927\n",
      "0.0067238432713488325\n",
      "0.006712177597176538\n",
      "0.006700550260080776\n",
      "0.006688961075496843\n",
      "0.006677409860028436\n",
      "0.006665896431438352\n",
      "0.006654420608639638\n",
      "0.006642982211686363\n",
      "0.006631581061764884\n",
      "0.006620216981185029\n",
      "0.006608889793371193\n",
      "0.006597599322853841\n",
      "0.006586345395260889\n",
      "0.006575127837309168\n",
      "0.006563946476796055\n",
      "0.0065528011425910904\n",
      "0.006541691664627666\n",
      "0.006530617873895041\n",
      "0.006519579602429892\n",
      "0.0065085766833086305\n",
      "0.006497608950639126\n",
      "0.006486676239553107\n",
      "0.006475778386198035\n",
      "0.006464915227729571\n",
      "0.006454086602303816\n",
      "0.006443292349069725\n",
      "0.006432532308161545\n",
      "0.006421806320691336\n",
      "0.006411114228741551\n",
      "0.006400455875357763\n",
      "0.006389831104541327\n",
      "0.006379239761242236\n",
      "0.0063686816913518796\n",
      "0.006358156741696047\n",
      "0.006347664760027854\n",
      "0.006337205595020937\n",
      "0.00632677909626231\n",
      "0.006316385114245781\n",
      "0.006306023500364971\n",
      "0.006295694106906819\n",
      "0.00628539678704475\n",
      "0.006275131394832216\n",
      "0.006264897785196126\n",
      "0.006254695813930258\n",
      "0.006244525337689035\n",
      "0.006234386213981065\n",
      "0.006224278301162815\n",
      "0.0062142014584323935\n",
      "0.0062041555458233755\n",
      "0.0061941404241986735\n",
      "0.006184155955244345\n",
      "0.006174202001463711\n",
      "0.006164278426171286\n",
      "0.006154385093486883\n",
      "0.006144521868329687\n",
      "0.006134688616412463\n",
      "0.0061248852042358835\n",
      "0.006115111499082589\n",
      "0.006105367369011709\n",
      "0.006095652682853135\n",
      "0.006085967310201995\n",
      "0.006076311121413018\n",
      "0.006066683987595266\n",
      "0.006057085780606496\n",
      "0.006047516373047812\n",
      "0.006037975638258404\n",
      "0.006028463450310236\n",
      "0.006018979684002704\n",
      "0.006009524214857526\n",
      "0.006000096919113539\n",
      "0.005990697673721621\n",
      "0.005981326356339554\n",
      "0.005971982845327019\n",
      "0.0059626670197406514\n",
      "0.005953378759329061\n",
      "0.005944117944527906\n",
      "0.005934884456455087\n",
      "0.005925678176905864\n",
      "0.005916498988348082\n",
      "0.005907346773917523\n",
      "0.005898221417413099\n",
      "0.005889122803292222\n",
      "0.005880050816666183\n",
      "0.0058710053432956145\n",
      "0.005861986269585834\n",
      "0.005852993482582504\n",
      "0.0058440268699669765\n",
      "0.005835086320052011\n",
      "0.005826171721777271\n",
      "0.005817282964705031\n",
      "0.0058084199390158525\n",
      "0.005799582535504265\n",
      "0.005790770645574518\n",
      "0.005781984161236425\n",
      "0.005773222975101022\n",
      "0.005764486980376623\n",
      "0.005755776070864605\n",
      "0.005747090140955301\n",
      "0.005738429085624034\n",
      "0.005729792800426976\n",
      "0.005721181181497342\n",
      "0.005712594125541277\n",
      "0.005704031529834073\n",
      "0.005695493292216214\n",
      "0.005686979311089507\n",
      "0.005678489485413294\n",
      "0.005670023714700698\n",
      "0.005661581899014794\n",
      "0.005653163938964904\n",
      "0.005644769735702909\n",
      "0.005636399190919593\n",
      "0.005628052206840992\n",
      "0.005619728686224739\n",
      "0.005611428532356598\n",
      "0.005603151649046725\n",
      "0.005594897940626365\n",
      "0.00558666731194417\n",
      "0.005578459668362883\n",
      "0.005570274915755772\n",
      "0.00556211296050331\n",
      "0.005553973709489726\n",
      "0.005545857070099646\n",
      "0.005537762950214803\n",
      "0.005529691258210781\n",
      "0.005521641902953574\n",
      "0.005513614793796532\n",
      "0.005505609840576944\n",
      "0.005497626953612943\n",
      "0.005489666043700339\n",
      "0.005481727022109441\n",
      "0.005473809800581879\n",
      "0.00546591429132765\n",
      "0.0054580404070218025\n",
      "0.005450188060801655\n",
      "0.005442357166263578\n",
      "0.005434547637460011\n",
      "0.005426759388896548\n",
      "0.005418992335528958\n",
      "0.005411246392760228\n",
      "0.005403521476437608\n",
      "0.005395817502849789\n",
      "0.005388134388724071\n",
      "0.005380472051223366\n",
      "0.005372830407943545\n",
      "0.005365209376910489\n",
      "0.005357608876577431\n",
      "0.005350028825822102\n",
      "0.005342469143944066\n",
      "0.005334929750661903\n",
      "0.005327410566110692\n",
      "0.005319911510839092\n",
      "0.005312432505806925\n",
      "0.005304973472382381\n",
      "0.005297534332339466\n",
      "0.005290115007855374\n",
      "0.00528271542150795\n",
      "0.005275335496273167\n",
      "0.005267975155522485\n",
      "0.005260634323020383\n",
      "0.005253312922921951\n",
      "0.005246010879770246\n",
      "0.005238728118493956\n",
      "0.005231464564404915\n",
      "0.0052242201431956636\n",
      "0.005216994780937149\n",
      "0.005209788404076136\n",
      "0.005202600939433041\n",
      "0.005195432314199495\n",
      "0.005188282455935968\n",
      "0.005181151292569562\n",
      "0.005174038752391559\n",
      "0.005166944764055355\n",
      "0.005159869256573982\n",
      "0.005152812159317963\n",
      "0.005145773402013065\n",
      "0.005138752914738166\n",
      "0.005131750627922895\n",
      "0.005124766472345537\n",
      "0.005117800379130961\n",
      "0.0051108522797482445\n",
      "0.0051039221060087915\n",
      "0.0050970097900640415\n",
      "0.00509011526440343\n",
      "0.005083238461852296\n",
      "0.00507637931556984\n",
      "0.005069537759046998\n",
      "0.0050627137261044786\n",
      "0.005055907150890682\n",
      "0.005049117967879713\n",
      "0.00504234611186942\n",
      "0.005035591517979267\n",
      "0.005028854121648621\n",
      "0.005022133858634538\n",
      "0.005015430665009981\n",
      "0.005008744477161878\n",
      "0.005002075231789182\n",
      "0.004995422865900906\n",
      "0.004988787316814431\n",
      "0.004982168522153439\n",
      "0.004975566419846176\n",
      "0.004968980948123524\n",
      "0.004962412045517302\n",
      "0.004955859650858305\n",
      "0.004949323703274583\n",
      "0.004942804142189677\n",
      "0.004936300907320716\n",
      "0.004929813938676784\n",
      "0.004923343176557104\n",
      "0.0049168885615493855\n",
      "0.004910450034527921\n",
      "0.004904027536652038\n",
      "0.004897621009364331\n",
      "0.004891230394388957\n",
      "0.00488485563373003\n",
      "0.004878496669669857\n",
      "0.004872153444767347\n",
      "0.004865825901856313\n",
      "0.004859513984044023\n",
      "0.004853217634709219\n",
      "0.0048469367975009035\n",
      "0.004840671416336488\n",
      "0.004834421435400333\n",
      "0.004828186799142095\n",
      "0.004821967452275183\n",
      "0.004815763339775268\n",
      "0.004809574406878693\n",
      "0.004803400599080888\n",
      "0.004797241862134948\n",
      "0.00479109814205011\n",
      "0.004784969385090165\n",
      "0.004778855537772075\n",
      "0.00477275654686446\n",
      "0.0047666723593860726\n",
      "0.004760602922604448\n",
      "0.004754548184034384\n",
      "0.004748508091436488\n",
      "0.004742482592815834\n",
      "0.00473647163642045\n",
      "0.004730475170740012\n",
      "0.004724493144504325\n",
      "0.004718525506682022\n",
      "0.004712572206479191\n",
      "0.004706633193337935\n",
      "0.004700708416935093\n",
      "0.004694797827180823\n",
      "0.004688901374217322\n",
      "0.004683019008417425\n",
      "0.004677150680383348\n",
      "0.0046712963409453415\n",
      "0.0046654559411603725\n",
      "0.004659629432310887\n",
      "0.004653816765903445\n",
      "0.004648017893667483\n",
      "0.00464223276755407\n",
      "0.004636461339734593\n",
      "0.004630703562599533\n",
      "0.004624959388757235\n",
      "0.004619228771032655\n",
      "0.0046135116624660905\n",
      "0.004607808016312086\n",
      "0.004602117786038078\n",
      "0.004596440925323292\n",
      "0.004590777388057509\n",
      "0.00458512712833981\n",
      "0.0045794901004775875\n",
      "0.004573866258985199\n",
      "0.004568255558582861\n",
      "0.004562657954195478\n",
      "0.004557073400951616\n",
      "0.004551501854182134\n",
      "0.004545943269419291\n",
      "0.004540397602395409\n",
      "0.00453486480904191\n",
      "0.004529344845488177\n",
      "0.004523837668060417\n",
      "0.004518343233280517\n",
      "0.004512861497865161\n",
      "0.004507392418724432\n",
      "0.004501935952961058\n",
      "0.004496492057869146\n",
      "0.0044910606909331855\n",
      "0.004485641809826966\n",
      "0.004480235372412616\n",
      "0.004474841336739452\n",
      "0.004469459661043048\n",
      "0.004464090303744156\n",
      "0.004458733223447642\n",
      "0.004453388378941616\n",
      "0.004448055729196278\n",
      "0.004442735233362998\n",
      "0.004437426850773347\n",
      "0.0044321305409380455\n",
      "0.004426846263546023\n",
      "0.004421573978463404\n",
      "0.004416313645732661\n",
      "0.004411065225571475\n",
      "0.004405828678371935\n",
      "0.004400603964699529\n",
      "0.004395391045292192\n",
      "0.004390189881059386\n",
      "0.004385000433081203\n",
      "0.004379822662607375\n",
      "0.0043746565310564004\n",
      "0.004369502000014672\n",
      "0.004364359031235464\n",
      "0.004359227586638098\n",
      "0.004354107628307112\n",
      "0.004348999118491216\n",
      "0.004343902019602602\n",
      "0.0043388162942158805\n",
      "0.0043337419050673304\n",
      "0.004328678815054017\n",
      "0.004323626987232845\n",
      "0.004318586384819882\n",
      "0.004313556971189323\n",
      "0.004308538709872751\n",
      "0.004303531564558229\n",
      "0.004298535499089601\n",
      "0.004293550477465516\n",
      "0.004288576463838633\n",
      "0.004283613422514918\n",
      "0.004278661317952695\n",
      "0.00427372011476189\n",
      "0.004268789777703291\n",
      "0.004263870271687649\n",
      "0.004258961561774896\n",
      "0.004254063613173485\n",
      "0.0042491763912394336\n",
      "0.004244299861475677\n",
      "0.004239433989531269\n",
      "0.004234578741200543\n",
      "0.0042297340824224315\n",
      "0.00422489997927967\n",
      "0.004220076397998061\n",
      "0.004215263304945761\n",
      "0.004210460666632403\n",
      "0.0042056684497085465\n",
      "0.0042008866209647736\n",
      "0.004196115147331075\n",
      "0.004191353995876092\n",
      "0.004186603133806366\n",
      "0.004181862528465655\n",
      "0.004177132147334173\n",
      "0.004172411958027977\n"
     ]
    }
   ],
   "source": [
    "# manual batch gradient descent\n",
    "learning_rate = 0.01\n",
    "n_iterations = 1000\n",
    "for _ in range(n_iterations):\n",
    "  # forward pass\n",
    "  ypred = [n(x) for x in xs]\n",
    "  loss = sum((ygt - yout)**2 for ygt, yout in zip(ys, ypred))\n",
    "  # backward pass\n",
    "  for p in n.parameters():\n",
    "    p.grad = 0.0\n",
    "  loss.backward()\n",
    "  # update\n",
    "  for p in n.parameters():\n",
    "    # I should be using `p.grad` instead of `p.grad/abs(p.grad)`. why?\n",
    "    # because the magnitude is valuable information: when it's small,\n",
    "    # I want the data update to also have a small magnitude so it doesn't\n",
    "    # overshoot the minima\n",
    "    p.data += -1 * learning_rate * p.grad\n",
    "  \n",
    "  print(loss.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.97452869211613),\n",
       " Value(data=-0.9752827214392236),\n",
       " Value(data=-0.9630843833158027),\n",
       " Value(data=0.9606310043551086)]"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
